{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/6vR+oK40TgAizcu3VN1M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PreetiKumari2208/web_development/blob/main/CONTLO_AI_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ku1TinMnF-wi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1 | GPT-2 Model & Checkpoints (20 Points)\n"
      ],
      "metadata": {
        "id": "dAFNpwRXGDnv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    # Implement the Multi-Head Attention module\n",
        "\n",
        "class PositionwiseFeedforward(nn.Module):\n",
        "    # Implement the Positionwise Feedforward module\n",
        "\n",
        "class TransformerLayer(nn.Module):\n",
        "    # Implement a single Transformer layer with Multi-Head Attention and Feedforward\n",
        "\n",
        "class GPT2(nn.Module):\n",
        "    # Implement the GPT-2 model using the Transformer architecture\n",
        "\n",
        "    def __init__(self):\n",
        "        super(GPT2, self).__init__()\n",
        "        # Initialize layers and parameters\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Implement the forward pass\n",
        "        return x\n",
        "\n",
        "# Sample usage:\n",
        "model = GPT2()\n",
        "input_sequence = torch.randint(0, vocab_size, (batch_size, sequence_length))\n",
        "output = model(input_sequence)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "dlD9iglbGK1V",
        "outputId": "274a44fd-670d-4747-8508-a4e15ea5200f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-2a94c0122ea1>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    class PositionwiseFeedforward(nn.Module):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after class definition on line 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2 | Transformer Architectural Changes (40 Points)\n"
      ],
      "metadata": {
        "id": "naZjK8EeGrY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class RotaryPositionalEmbedding(nn.Module):\n",
        "    # Implement the Rotary Positional Embedding\n",
        "\n",
        "class GroupQueryAttention(nn.Module):\n",
        "    # Implement the Group Query Attention mechanism\n",
        "\n",
        "class SlidingWindowAttention(nn.Module):\n",
        "    # Implement the Sliding Window Attention mechanism\n",
        "\n",
        "class GPT2WithAlterations(nn.Module):\n",
        "    def __init__(self, use_rotary_pos_emb=False, use_group_query_att=False, use_sliding_window_att=False):\n",
        "        super(GPT2WithAlterations, self).__init__()\n",
        "\n",
        "        # Original GPT-2 layers (token embedding, transformer layers, etc.)\n",
        "        self.token_embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.transformer_layers = nn.ModuleList([TransformerLayer() for _ in range(num_layers)])\n",
        "\n",
        "        # Add alterations based on input flags\n",
        "        if use_rotary_pos_emb:\n",
        "            self.rotary_pos_emb = RotaryPositionalEmbedding()\n",
        "        if use_group_query_att:\n",
        "            self.group_query_att = GroupQueryAttention()\n",
        "        if use_sliding_window_att:\n",
        "            self.sliding_window_att = SlidingWindowAttention()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Original GPT-2 forward pass\n",
        "        token_embed = self.token_embedding(x)\n",
        "        for layer in self.transformer_layers:\n",
        "            token_embed = layer(token_embed)\n",
        "\n",
        "        # Apply alterations if specified\n",
        "        if hasattr(self, 'rotary_pos_emb'):\n",
        "            token_embed = self.rotary_pos_emb(token_embed)\n",
        "        if hasattr(self, 'group_query_att'):\n",
        "            token_embed = self.group_query_att(token_embed)\n",
        "        if hasattr(self, 'sliding_window_att'):\n",
        "            token_embed = self.sliding_window_att(token_embed)\n",
        "\n",
        "        return token_embed\n",
        "\n",
        "# Instantiate the model with desired alterations\n",
        "model = GPT2WithAlterations(use_rotary_pos_emb=True, use_group_query_att=True, use_sliding_window_att=True)\n",
        "\n",
        "# Sample usage:\n",
        "input_sequence = torch.randint(0, vocab_size, (batch_size, sequence_length))\n",
        "output = model(input_sequence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "6SfdOkbHGzMj",
        "outputId": "b0854e77-53ec-4aa2-fa43-8eeab1b0d794"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-b39ade83b88f>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    class GroupQueryAttention(nn.Module):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after class definition on line 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 3: Training Loop Implementation (40 Points)\n"
      ],
      "metadata": {
        "id": "Q4DgYlAfHPBf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.parallel import DistributedDataParallel\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from fsdp import FullyShardedDataParallel\n",
        "\n",
        "# Assuming you have a GPT2 model as GPT2WithAlterations from Task 2\n",
        "model = GPT2WithAlterations(use_rotary_pos_emb=True, use_group_query_att=True, use_sliding_window_att=True)\n",
        "model = model.to(device)\n",
        "\n",
        "# Define your loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Define your DataLoader\n",
        "# ...\n",
        "\n",
        "# Training loop for a single GPU\n",
        "def train_single_gpu(model, dataloader, criterion, optimizer):\n",
        "    model.train()\n",
        "    for inputs, targets in dataloader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Training loop for DistributedDataParallel (DDP)\n",
        "def train_ddp(model, dataloader, criterion, optimizer, device, rank, world_size):\n",
        "    model = DistributedDataParallel(model, device_ids=[rank])\n",
        "    model = model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    for inputs, targets in dataloader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Training loop for Fully Sharded Data Parallel (FSDP)\n",
        "def train_fsdp(model, dataloader, criterion, optimizer, device):\n",
        "    fsdp_model = FullyShardedDataParallel(model)\n",
        "    fsdp_model = fsdp_model.to(device)\n",
        "    fsdp_model.train()\n",
        "\n",
        "    for inputs, targets in dataloader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = fsdp_model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Example usage\n",
        "train_single_gpu(model, train_dataloader, criterion, optimizer)\n",
        "# For DDP\n",
        "# train_ddp(model, train_dataloader, criterion, optimizer, device, rank, world_size)\n",
        "# For FSDP\n",
        "# train_fsdp(model, train_dataloader, criterion, optimizer, device)\n"
      ],
      "metadata": {
        "id": "guaHgiMhHm9w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}